# Manual Test Script

Run Atlas crawler against real websites to validate functionality and stealth plugin effectiveness.

## Usage

### 1. Configure Test URLs

Edit `scripts/manual-test.ts` and add your target sites to the `TEST_SITES` array:

```typescript
const TEST_SITES = [
  {
    name: 'Example.com',
    url: 'https://example.com',
    maxPages: 5,
    maxDepth: 1,
  },
  {
    name: 'Your Site',
    url: 'https://your-site.com',
    maxPages: 10,
    maxDepth: 2,
  },
];
```

### 2. Run the Test

```bash
npm run test:manual
```

### 3. Review Results

The script will:
- ‚úÖ Crawl each site sequentially
- üìä Print detailed statistics for each crawl
- üíæ Save results to `output/` directory as JSON
- üìà Display a summary at the end

## Output Example

```
üöÄ Atlas Manual Test Runner
============================

Testing 2 site(s)...

üìç Testing: Example.com (https://example.com)
   Settings: maxPages=5, maxDepth=1
   ‚úÖ Success!
   ‚è±Ô∏è  Duration: 12.34s
   üìÑ Pages: 3
   üñºÔ∏è  Assets: 12
   üìä State: 3 visited, 0 failed
   üíæ Output: output/example-com-2025-12-14T18-30-00-000Z.json

   Sample Page (https://example.com):
   - Title: Example Domain
   - Links: 2 (1 internal)
   - Assets: 4
   - Text length: 1256 chars
   - HTML length: 8432 chars
   - Structured data: 1 JSON-LD, 0 microdata

üìä SUMMARY
==========

Total: 2 tests
‚úÖ Passed: 2
‚ùå Failed: 0

Successful tests:
  ‚Ä¢ Example.com: 3 pages, 12 assets (12.34s)
    Output: output/example-com-2025-12-14T18-30-00-000Z.json
  ‚Ä¢ Your Site: 8 pages, 45 assets (23.45s)
    Output: output/your-site-com-2025-12-14T18-30-01-000Z.json

‚ú® Done!
```

## What to Look For

### Success Indicators
- ‚úÖ Crawl completes without errors
- ‚úÖ Pages extracted (count > 0)
- ‚úÖ HTML and text content populated
- ‚úÖ Metadata extracted (title, description)
- ‚úÖ Links discovered
- ‚úÖ Assets found
- ‚úÖ No bot detection errors

### Common Issues
- ‚ùå "Navigation timeout" - Site too slow or blocking
- ‚ùå "Access denied" - Bot detection triggered
- ‚ùå "SSL certificate error" - Invalid HTTPS cert
- ‚ùå "Cannot navigate to invalid URL" - Bad URL format

## Tips

- **Start small**: Test with `maxPages: 5` first
- **Increase gradually**: If successful, try larger crawls
- **Monitor output**: Check JSON files for data quality
- **Test your own sites**: Validate against sites you control
- **Avoid rate limits**: Don't crawl the same site repeatedly in short intervals

## Configuration Options

Each test site supports these options:

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `name` | string | - | Friendly name for output |
| `url` | string | - | Target URL to crawl |
| `maxPages` | number | 100 | Max pages to crawl |
| `maxDepth` | number | 3 | Max depth from seed |
| `timeout` | number | 30000 | Request timeout (ms) |

## Troubleshooting

**Script crashes immediately**
- Check URL format (must be `https://` or `http://`)
- Ensure site is accessible in browser first

**No pages crawled**
- Site may be blocking headless browsers
- Try `headless: false` in script to debug
- Check if site has robots.txt restrictions

**Slow performance**
- Lower `concurrency` if site rate-limits
- Increase `timeout` for slow sites
- Reduce `maxPages` to test faster

**Memory issues**
- Lower `maxPages` to reduce memory usage
- Run tests one at a time (comment out others)
- Close other applications
